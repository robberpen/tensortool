#!/usr/bin/env python3
"""
TensorFlow Lite Runner for DSP Development
Author: DSP AI Expert
Version: 1.0

A strict, DSP-oriented TensorFlow Lite inference tool with quantization error analysis.
Designed for Qualcomm/MediaTek ASOC processor development workflow.
"""

import argparse
import sys
import os
import numpy as np
import tensorflow as tf
from typing import Optional, Tuple, Dict, Any
import json


class ModelComparator:
    """Quantization error analysis for DSP development"""
    
    def __init__(self):
        self.metrics = {}
    
    def calculate_mse(self, reference: np.ndarray, quantized: np.ndarray) -> float:
        """Calculate Mean Squared Error"""
        if reference.shape != quantized.shape:
            raise ValueError(f"Shape mismatch: {reference.shape} vs {quantized.shape}")
        
        mse = np.mean((reference.astype(np.float64) - quantized.astype(np.float64)) ** 2)
        return float(mse)
    
    def calculate_mae(self, reference: np.ndarray, quantized: np.ndarray) -> float:
        """Calculate Mean Absolute Error"""
        if reference.shape != quantized.shape:
            raise ValueError(f"Shape mismatch: {reference.shape} vs {quantized.shape}")
        
        mae = np.mean(np.abs(reference.astype(np.float64) - quantized.astype(np.float64)))
        return float(mae)
    
    def compare_outputs(self, reference: np.ndarray, quantized: np.ndarray) -> Dict[str, float]:
        """Compare reference vs quantized outputs"""
        try:
            mse = self.calculate_mse(reference, quantized)
            mae = self.calculate_mae(reference, quantized)
            
            # Additional DSP-relevant metrics
            max_error = np.max(np.abs(reference - quantized))
            snr_db = 10 * np.log10(np.mean(reference**2) / (mse + 1e-10))
            
            metrics = {
                'mse': mse,
                'mae': mae, 
                'max_absolute_error': float(max_error),
                'snr_db': float(snr_db)
            }
            
            return metrics
            
        except Exception as e:
            print(f"ERROR: Failed to compare outputs - {e}")
            sys.exit(1)


class TFLiteRunner:
    """TensorFlow Lite inference runner for DSP development"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.comparator = ModelComparator()
        
        # Validate model file
        if not os.path.exists(model_path):
            print(f"ERROR: Model file not found: {model_path}")
            sys.exit(1)
        
        if not model_path.endswith('.tflite'):
            print(f"ERROR: Invalid model file extension. Expected .tflite, got: {model_path}")
            sys.exit(1)
    
    def load_model(self) -> None:
        """Load TensorFlow Lite model with strict validation"""
        try:
            self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
            self.interpreter.allocate_tensors()
            
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            print(f"SUCCESS: Model loaded - {self.model_path}")
            
        except Exception as e:
            print(f"ERROR: Failed to load model - {e}")
            sys.exit(1)
    
    def print_model_info(self) -> None:
        """Print comprehensive model information for DSP analysis"""
        if not self.interpreter:
            print("ERROR: Model not loaded")
            sys.exit(1)
        
        print("\n" + "="*60)
        print("TensorFlow Lite Model Analysis")
        print("="*60)
        
        # Model metadata
        print(f"Model Path: {self.model_path}")
        print(f"Model Size: {os.path.getsize(self.model_path):,} bytes")
        
        # Input tensors analysis
        print(f"\nInput Tensors: {len(self.input_details)}")
        for i, input_detail in enumerate(self.input_details):
            print(f"  [{i}] Name: {input_detail['name']}")
            print(f"      Shape: {input_detail['shape']}")
            print(f"      DType: {input_detail['dtype']}")
            print(f"      Index: {input_detail['index']}")
            
            # Quantization info if available
            if 'quantization_parameters' in input_detail:
                quant_params = input_detail['quantization_parameters']
                if quant_params['scales'].size > 0:
                    print(f"      Quantization Scale: {quant_params['scales'][0]}")
                    print(f"      Quantization Zero Point: {quant_params['zero_points'][0]}")
        
        # Output tensors analysis  
        print(f"\nOutput Tensors: {len(self.output_details)}")
        for i, output_detail in enumerate(self.output_details):
            print(f"  [{i}] Name: {output_detail['name']}")
            print(f"      Shape: {output_detail['shape']}")
            print(f"      DType: {output_detail['dtype']}")
            print(f"      Index: {output_detail['index']}")
            
            # Quantization info if available
            if 'quantization_parameters' in output_detail:
                quant_params = output_detail['quantization_parameters']
                if quant_params['scales'].size > 0:
                    print(f"      Quantization Scale: {quant_params['scales'][0]}")
                    print(f"      Quantization Zero Point: {quant_params['zero_points'][0]}")
        
        print("\n" + "="*60)
    
    def auto_reshape_input(self, input_data: np.ndarray, expected_shape: Tuple) -> np.ndarray:
        """Automatic reshape with common format conversions (USE WITH CAUTION!)"""
        current_shape = input_data.shape
        
        print(f"WARNING: Attempting automatic reshape")
        print(f"         From: {current_shape} -> To: {expected_shape}")
        
        # Common case: HWC -> NCHW for image data
        if (len(current_shape) == 3 and len(expected_shape) == 4 and
            current_shape[2] == expected_shape[1] and  # channels match
            current_shape[0] == expected_shape[2] and  # height match  
            current_shape[1] == expected_shape[3] and  # width match
            expected_shape[0] == 1):  # batch size = 1
            
            print(f"INFO: Detected HWC->NCHW conversion")
            print(f"      HWC: (H={current_shape[0]}, W={current_shape[1]}, C={current_shape[2]})")
            print(f"      NCHW: (N={expected_shape[0]}, C={expected_shape[1]}, H={expected_shape[2]}, W={expected_shape[3]})")
            
            # HWC -> CHW -> NCHW
            reshaped = np.transpose(input_data, (2, 0, 1))  # HWC -> CHW
            reshaped = np.expand_dims(reshaped, axis=0)     # CHW -> NCHW
            
            print(f"SUCCESS: Auto-reshaped to {reshaped.shape}")
            return reshaped
        
        # Common case: CHW -> NCHW (add batch dimension)
        elif (len(current_shape) == 3 and len(expected_shape) == 4 and
              current_shape == expected_shape[1:] and
              expected_shape[0] == 1):
            
            print(f"INFO: Detected CHW->NCHW conversion (adding batch dimension)")
            reshaped = np.expand_dims(input_data, axis=0)
            print(f"SUCCESS: Auto-reshaped to {reshaped.shape}")
            return reshaped
        
        # Simple reshape if total elements match
        elif np.prod(current_shape) == np.prod(expected_shape):
            print(f"WARNING: Simple reshape - verify data semantics!")
            reshaped = input_data.reshape(expected_shape)
            print(f"SUCCESS: Reshaped to {reshaped.shape}")
            return reshaped
        
        else:
            print(f"ERROR: Cannot auto-reshape {current_shape} to {expected_shape}")
            print(f"       Total elements: {np.prod(current_shape)} vs {np.prod(expected_shape)}")
            print(f"       Supported conversions:")
            print(f"       - HWC -> NCHW (image format)")
            print(f"       - CHW -> NCHW (add batch)")  
            print(f"       - Same total elements (simple reshape)")
            sys.exit(1)

    def auto_type_convert(self, input_data: np.ndarray, expected_dtype: np.dtype) -> np.ndarray:
        """Automatic type conversion with DSP-safe strategies (USE WITH EXTREME CAUTION!)"""
        current_dtype = input_data.dtype
        
        print(f"WARNING: Attempting automatic type conversion")
        print(f"         From: {current_dtype} -> To: {expected_dtype}")
        print(f"         Data range: [{input_data.min():.6f}, {input_data.max():.6f}]")
        
        # Strategy 1: float32 -> uint8 (common for image data)
        if current_dtype == np.float32 and expected_dtype == np.uint8:
            print(f"INFO: Detected float32->uint8 conversion")
            
            # Check if data is normalized [0,1]
            if input_data.min() >= 0.0 and input_data.max() <= 1.0:
                print(f"      Strategy: Normalized float [0,1] -> uint8 [0,255]")
                converted = (input_data * 255.0).astype(np.uint8)
                print(f"      Result range: [{converted.min()}, {converted.max()}]")
                return converted
            
            # Check if data is in [-1,1] range
            elif input_data.min() >= -1.0 and input_data.max() <= 1.0:
                print(f"      Strategy: Normalized float [-1,1] -> uint8 [0,255]")
                converted = ((input_data + 1.0) * 127.5).astype(np.uint8)
                print(f"      Result range: [{converted.min()}, {converted.max()}]")
                return converted
            
            # Check if data is already in [0,255] range
            elif input_data.min() >= 0.0 and input_data.max() <= 255.0:
                print(f"      Strategy: Direct float32->uint8 cast")
                converted = input_data.astype(np.uint8)
                print(f"      Result range: [{converted.min()}, {converted.max()}]")
                return converted
            
            else:
                print(f"ERROR: Cannot safely convert float32 to uint8")
                print(f"       Data range [{input_data.min():.6f}, {input_data.max():.6f}] is outside safe bounds")
                print(f"       Safe ranges: [0,1], [-1,1], or [0,255]")
                sys.exit(1)
        
        # Strategy 2: uint8 -> float32 (always safe)
        elif current_dtype == np.uint8 and expected_dtype == np.float32:
            print(f"INFO: Detected uint8->float32 conversion (always safe)")
            converted = input_data.astype(np.float32)
            print(f"      Result range: [{converted.min():.1f}, {converted.max():.1f}]")
            return converted
        
        # Strategy 3: int8 -> uint8 (with offset)
        elif current_dtype == np.int8 and expected_dtype == np.uint8:
            print(f"INFO: Detected int8->uint8 conversion")
            print(f"      Strategy: Add 128 offset to shift range")
            converted = (input_data.astype(np.int16) + 128).astype(np.uint8)
            print(f"      Result range: [{converted.min()}, {converted.max()}]")
            return converted
        
        # Strategy 4: uint8 -> int8 (with offset)
        elif current_dtype == np.uint8 and expected_dtype == np.int8:
            print(f"INFO: Detected uint8->int8 conversion")
            print(f"      Strategy: Subtract 128 offset to shift range")
            converted = (input_data.astype(np.int16) - 128).astype(np.int8)
            print(f"      Result range: [{converted.min()}, {converted.max()}]")
            return converted
        
        # Strategy 5: Same bit-width conversions (risky!)
        elif (current_dtype == np.float32 and expected_dtype == np.int32) or \
             (current_dtype == np.int32 and expected_dtype == np.float32):
            print(f"WARNING: Same bit-width conversion - verify semantics!")
            converted = input_data.astype(expected_dtype)
            print(f"         Result range: [{converted.min()}, {converted.max()}]")
            return converted
        
        else:
            print(f"ERROR: Unsupported type conversion {current_dtype} -> {expected_dtype}")
            print(f"       Supported conversions:")
            print(f"       - float32 -> uint8 (normalized data)")
            print(f"       - uint8 -> float32 (always safe)")
            print(f"       - int8 <-> uint8 (with offset)")
            print(f"       HINT: Pre-process your data to match expected type")
            sys.exit(1)

    def validate_input_compatibility(self, input_data: np.ndarray, target_shape: Optional[Tuple] = None, auto_reshape: bool = False, auto_type: bool = False) -> np.ndarray:
        """Input validation with optional automatic reshaping"""
        if not self.input_details:
            print("ERROR: Model not loaded")
            sys.exit(1)
        
        expected_shape = tuple(self.input_details[0]['shape'])
        expected_dtype = self.input_details[0]['dtype']
        
        print(f"DEBUG: Input validation")
        print(f"       Expected shape: {expected_shape}")
        print(f"       Expected dtype: {expected_dtype}")
        print(f"       Actual shape: {input_data.shape}")
        print(f"       Actual dtype: {input_data.dtype}")
        
        # Shape validation
        if target_shape:
            target_shape = tuple(target_shape)
            if input_data.shape != target_shape:
                print(f"ERROR: Input shape mismatch")
                print(f"       Target shape: {target_shape}")
                print(f"       Input shape: {input_data.shape}")
                sys.exit(1)
            
            if target_shape != expected_shape:
                print(f"ERROR: Target shape doesn't match model expected shape")
                print(f"       Expected: {expected_shape}")
                print(f"       Target: {target_shape}")
                sys.exit(1)
        else:
            if input_data.shape != expected_shape:
                if auto_reshape:
                    input_data = self.auto_reshape_input(input_data, expected_shape)
                else:
                    print(f"ERROR: Input shape mismatch with model")
                    print(f"       Expected: {expected_shape}")
                    print(f"       Actual: {input_data.shape}")
                    print(f"       HINT: Use --auto_reshape for automatic conversion")
                    print(f"       HINT: Common conversions:")
                    if len(input_data.shape) == 3 and len(expected_shape) == 4:
                        print(f"             HWC->NCHW: np.transpose(data, (2,0,1))[np.newaxis,...]")
                        print(f"             CHW->NCHW: np.expand_dims(data, axis=0)")
                    sys.exit(1)
        
        # Type validation with optional auto conversion
        if input_data.dtype != expected_dtype:
            if auto_type:
                input_data = self.auto_type_convert(input_data, expected_dtype)
            else:
                print(f"ERROR: Input dtype mismatch")
                print(f"       Expected: {expected_dtype}")
                print(f"       Actual: {input_data.dtype}")
                print(f"       HINT: Use --auto_type for automatic conversion")
                print(f"       HINT: Common conversions:")
                if input_data.dtype == np.float32 and expected_dtype == np.uint8:
                    print(f"             float32->uint8: (data * 255).astype(np.uint8)  # if data in [0,1]")
                elif input_data.dtype == np.uint8 and expected_dtype == np.float32:
                    print(f"             uint8->float32: data.astype(np.float32)")
                print(f"       DSP Rule: Explicit type handling preferred")
                sys.exit(1)
        
        return input_data
    
    def normalize_input(self, input_data: np.ndarray, normalize_mode: str) -> np.ndarray:
        """Apply normalization with DSP-safe operations"""
        if normalize_mode == "0,1":
            # Convert to float32 first for normalization
            if input_data.dtype == np.uint8:
                normalized = input_data.astype(np.float32) / 255.0
                print(f"DEBUG: Normalized [0,1] - min: {normalized.min():.6f}, max: {normalized.max():.6f}")
                return normalized
            else:
                print(f"ERROR: Normalization [0,1] only supports uint8 input, got {input_data.dtype}")
                sys.exit(1)
                
        elif normalize_mode == "-1,1":
            # Convert to float32 first for normalization  
            if input_data.dtype == np.uint8:
                normalized = (input_data.astype(np.float32) / 255.0) * 2.0 - 1.0
                print(f"DEBUG: Normalized [-1,1] - min: {normalized.min():.6f}, max: {normalized.max():.6f}")
                return normalized
            else:
                print(f"ERROR: Normalization [-1,1] only supports uint8 input, got {input_data.dtype}")
                sys.exit(1)
        else:
            print(f"ERROR: Unsupported normalization mode: {normalize_mode}")
            sys.exit(1)
    
    def run_inference(self, input_data: np.ndarray) -> np.ndarray:
        """Execute model inference with error checking"""
        try:
            # Set input tensor
            self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
            
            # Run inference
            self.interpreter.invoke()
            
            # Get output tensor
            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
            
            print(f"DEBUG: Inference completed")
            print(f"       Output shape: {output_data.shape}")
            print(f"       Output dtype: {output_data.dtype}")
            print(f"       Output range: [{output_data.min():.6f}, {output_data.max():.6f}]")
            
            return output_data
            
        except Exception as e:
            print(f"ERROR: Inference failed - {e}")
            sys.exit(1)


def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments with DSP-specific options"""
    parser = argparse.ArgumentParser(
        description="TensorFlow Lite Runner for DSP Development",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Display model information
  python tflite_runner.py --model model.tflite
  
  # Run inference with input file  
  python tflite_runner.py --model model.tflite --in_path input.npy
  
  # Run with normalization and save output
  python tflite_runner.py --model model.tflite --in_path input.npy --out_path output.npy --normalize "0,1"
  
  # Compare with reference model
  python tflite_runner.py --model quantized.tflite --in_path input.npy --ref_model float32.tflite
        """
    )
    
    parser.add_argument('--model', required=True, 
                       help='Path to TensorFlow Lite model (.tflite)')
    parser.add_argument('--in_path', 
                       help='Input tensor file (.npy)')
    parser.add_argument('--out_path',
                       help='Output tensor file (.npy)')
    parser.add_argument('--in_shape',
                       help='Input tensor shape (e.g., 640,640,3)')
    parser.add_argument('--normalize',
                       help='Normalize input: "0,1" or "-1,1"')
    parser.add_argument('--auto_reshape', action='store_true',
                       help='Enable automatic reshape for common format conversions (HWC<->NCHW)')
    parser.add_argument('--auto_type', action='store_true',
                       help='Enable automatic type conversion with DSP-safe strategies')
    parser.add_argument('--ref_model',
                       help='Reference float32 model for error analysis')
    
    return parser.parse_args()


def load_numpy_file(file_path: str) -> np.ndarray:
    """Load numpy file with validation"""
    try:
        if not os.path.exists(file_path):
            print(f"ERROR: Input file not found: {file_path}")
            sys.exit(1)
        
        data = np.load(file_path)
        print(f"DEBUG: Loaded {file_path}")
        print(f"       Shape: {data.shape}")
        print(f"       DType: {data.dtype}")
        print(f"       Range: [{data.min()}, {data.max()}]")
        
        return data
        
    except Exception as e:
        print(f"ERROR: Failed to load numpy file {file_path} - {e}")
        sys.exit(1)


def save_numpy_file(data: np.ndarray, file_path: str) -> None:
    """Save numpy file with validation"""
    try:
        np.save(file_path, data)
        print(f"SUCCESS: Output saved to {file_path}")
        print(f"         Shape: {data.shape}")
        print(f"         DType: {data.dtype}")
        
    except Exception as e:
        print(f"ERROR: Failed to save numpy file {file_path} - {e}")
        sys.exit(1)


def parse_shape(shape_str: str) -> Tuple[int, ...]:
    """Parse shape string to tuple"""
    try:
        shape = tuple(map(int, shape_str.split(',')))
        return shape
    except Exception as e:
        print(f"ERROR: Invalid shape format: {shape_str} - {e}")
        print("       Expected format: 640,640,3")
        sys.exit(1)


def main():
    """Main execution function"""
    args = parse_arguments()
    
    # Initialize runner
    runner = TFLiteRunner(args.model)
    runner.load_model()
    
    # If no input specified, just show model info
    if not args.in_path:
        runner.print_model_info()
        return
    
    # Load and validate input
    input_data = load_numpy_file(args.in_path)
    
    # Parse target shape if specified
    target_shape = None
    if args.in_shape:
        target_shape = parse_shape(args.in_shape)
    
    # Apply normalization if specified
    if args.normalize:
        input_data = runner.normalize_input(input_data, args.normalize)
    
    # Validate input compatibility
    input_data = runner.validate_input_compatibility(input_data, target_shape, args.auto_reshape, args.auto_type)
    
    # Run inference
    output_data = runner.run_inference(input_data)
    
    # Compare with reference model if specified
    if args.ref_model:
        print("\n" + "="*50)
        print("QUANTIZATION ERROR ANALYSIS")
        print("="*50)
        
        # Load reference model
        ref_runner = TFLiteRunner(args.ref_model)
        ref_runner.load_model()
        
        # Get reference input (before normalization for fair comparison)
        ref_input = load_numpy_file(args.in_path)
        if target_shape:
            ref_input = ref_runner.validate_input_compatibility(ref_input, target_shape, args.auto_reshape, args.auto_type)
        else:
            ref_input = ref_runner.validate_input_compatibility(ref_input, None, args.auto_reshape, args.auto_type)
        
        # Apply same normalization to reference
        if args.normalize:
            ref_input = ref_runner.normalize_input(ref_input, args.normalize)
        
        # Run reference inference
        ref_output = ref_runner.run_inference(ref_input)
        
        # Compare outputs
        metrics = runner.comparator.compare_outputs(ref_output, output_data)
        
        print(f"Reference Model: {args.ref_model}")
        print(f"Quantized Model: {args.model}")
        print(f"MSE: {metrics['mse']:.8f}")
        print(f"MAE: {metrics['mae']:.8f}")
        print(f"Max Absolute Error: {metrics['max_absolute_error']:.8f}")
        print(f"SNR (dB): {metrics['snr_db']:.2f}")
        
        if metrics['mse'] > 1e-3:
            print("WARNING: High quantization error detected!")
        
        print("="*50)
    
    # Save output if specified
    if args.out_path:
        save_numpy_file(output_data, args.out_path)
    else:
        # Print to stdout for pipeline usage
        print(f"\nInference Result:")
        print(f"Shape: {output_data.shape}")
        print(f"DType: {output_data.dtype}")
        print(f"Range: [{output_data.min():.6f}, {output_data.max():.6f}]")


if __name__ == "__main__":
    main()
